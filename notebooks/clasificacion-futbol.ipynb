{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2999734,"sourceType":"datasetVersion","datasetId":1826242}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-12T03:57:10.023936Z","iopub.execute_input":"2025-12-12T03:57:10.024239Z","iopub.status.idle":"2025-12-12T03:57:19.419065Z","shell.execute_reply.started":"2025-12-12T03:57:10.024218Z","shell.execute_reply":"2025-12-12T03:57:19.418335Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms, models\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n\n# Mostrar versiones\nprint(\"PyTorch:\", torch.__version__)\n\n# Fijar semilla para reproducibilidad\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T03:57:19.420167Z","iopub.execute_input":"2025-12-12T03:57:19.420511Z","iopub.status.idle":"2025-12-12T03:57:26.986765Z","shell.execute_reply.started":"2025-12-12T03:57:19.420490Z","shell.execute_reply":"2025-12-12T03:57:26.985967Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ver qué hay en /kaggle/input\nprint(\"Football-Tackes\")\n!ls /kaggle/input\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T03:57:26.987591Z","iopub.execute_input":"2025-12-12T03:57:26.987989Z","iopub.status.idle":"2025-12-12T03:57:27.121793Z","shell.execute_reply.started":"2025-12-12T03:57:26.987961Z","shell.execute_reply":"2025-12-12T03:57:27.121087Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATASET_ROOT = \"/kaggle/input/football-tackles/var400/VAR\"  # o var_200/VAR si quieres imágenes más pequeñas\n\nprint(\"DATASET_ROOT:\", DATASET_ROOT)\n!ls \"$DATASET_ROOT\"\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T03:57:27.123664Z","iopub.execute_input":"2025-12-12T03:57:27.123975Z","iopub.status.idle":"2025-12-12T03:57:27.256039Z","shell.execute_reply.started":"2025-12-12T03:57:27.123950Z","shell.execute_reply":"2025-12-12T03:57:27.255294Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_dataset = datasets.ImageFolder(DATASET_ROOT)\nclass_names = full_dataset.classes\nnum_classes = len(class_names)\n\nprint(\"Clases:\", class_names)\nprint(\"Total de imágenes:\", len(full_dataset))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T03:57:27.257080Z","iopub.execute_input":"2025-12-12T03:57:27.257311Z","iopub.status.idle":"2025-12-12T03:57:27.637966Z","shell.execute_reply.started":"2025-12-12T03:57:27.257284Z","shell.execute_reply":"2025-12-12T03:57:27.637235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMG_SIZE = 224  # tamaño típico para ResNet\n\ndata_transforms = {\n    \"train\": transforms.Compose([\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomRotation(10),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n        ),\n    ]),\n    \"val\": transforms.Compose([\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n        ),\n    ]),\n}\n\n# Dataset base solo para leer clases y tamaño\nfull_dataset = datasets.ImageFolder(DATASET_ROOT)\nclass_names = full_dataset.classes\nnum_classes = len(class_names)\n\nprint(\"Clases:\", class_names)\nprint(\"Total de imágenes:\", len(full_dataset))\n\n# indices aleatorios para train / val (80% / 20%)\nindices = torch.randperm(len(full_dataset)).tolist()\ntrain_size = int(0.8 * len(full_dataset))\ntrain_indices = indices[:train_size]\nval_indices = indices[train_size:]\n\n# Dataset con transform de train\ntrain_dataset = Subset(\n    datasets.ImageFolder(DATASET_ROOT, transform=data_transforms[\"train\"]),\n    train_indices\n)\n\n# Dataset con transform de validación\nval_dataset = Subset(\n    datasets.ImageFolder(DATASET_ROOT, transform=data_transforms[\"val\"]),\n    val_indices\n)\n\nprint(\"Tamaño train:\", len(train_dataset))\nprint(\"Tamaño val:\", len(val_dataset))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T03:57:27.638749Z","iopub.execute_input":"2025-12-12T03:57:27.639004Z","iopub.status.idle":"2025-12-12T03:57:27.672159Z","shell.execute_reply.started":"2025-12-12T03:57:27.638986Z","shell.execute_reply":"2025-12-12T03:57:27.671391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 32  # si estás sin GPU y se cuelga, bájalo a 16 o 8\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T03:57:27.672992Z","iopub.execute_input":"2025-12-12T03:57:27.673329Z","iopub.status.idle":"2025-12-12T03:57:27.677878Z","shell.execute_reply.started":"2025-12-12T03:57:27.673311Z","shell.execute_reply":"2025-12-12T03:57:27.677189Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cargar ResNet18 con pesos de ImageNet\nweights = models.ResNet18_Weights.IMAGENET1K_V1\nmodel = models.resnet18(weights=weights)\n\n# Cambiar la última capa para 2 clases (Clean_Tackles, Fouls)\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, num_classes)\n\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\nprint(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T03:57:27.678527Z","iopub.execute_input":"2025-12-12T03:57:27.678739Z","iopub.status.idle":"2025-12-12T03:57:28.542502Z","shell.execute_reply.started":"2025-12-12T03:57:27.678724Z","shell.execute_reply":"2025-12-12T03:57:28.541840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"NUM_EPOCHS = 12  # con GPU puedes subir a 15–20, con CPU déjalo en 8–12\n\nhistory = {\n    \"train_loss\": [],\n    \"val_loss\": [],\n    \"train_acc\": [],\n    \"val_acc\": [],\n}\n\nbest_acc = 0.0\nbest_state_dict = None\n\nfor epoch in range(NUM_EPOCHS):\n    # --- TRAIN ---\n    model.train()\n    running_loss = 0.0\n    running_corrects = 0\n    total_train = 0\n\n    for inputs, labels in train_loader:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n        optimizer.step()\n\n        _, preds = torch.max(outputs, 1)\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += (preds == labels).sum().item()\n        total_train += labels.size(0)\n\n    epoch_train_loss = running_loss / total_train\n    epoch_train_acc = running_corrects / total_train\n\n    # --- VALIDATION ---\n    model.eval()\n    val_loss = 0.0\n    val_corrects = 0\n    total_val = 0\n\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            _, preds = torch.max(outputs, 1)\n            val_loss += loss.item() * inputs.size(0)\n            val_corrects += (preds == labels).sum().item()\n            total_val += labels.size(0)\n\n    epoch_val_loss = val_loss / total_val\n    epoch_val_acc = val_corrects / total_val\n\n    history[\"train_loss\"].append(epoch_train_loss)\n    history[\"val_loss\"].append(epoch_val_loss)\n    history[\"train_acc\"].append(epoch_train_acc)\n    history[\"val_acc\"].append(epoch_val_acc)\n\n    if epoch_val_acc > best_acc:\n        best_acc = epoch_val_acc\n        best_state_dict = model.state_dict()\n\n    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] \"\n          f\"Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f} | \"\n          f\"Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}\")\n\n# Guardar mejores pesos\nos.makedirs(\"weights\", exist_ok=True)\ntorch.save(best_state_dict, \"weights/model_best.pth\")\nprint(f\"Mejor accuracy en validación: {best_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T03:57:28.543402Z","iopub.execute_input":"2025-12-12T03:57:28.543685Z","iopub.status.idle":"2025-12-12T03:58:30.465456Z","shell.execute_reply.started":"2025-12-12T03:57:28.543660Z","shell.execute_reply":"2025-12-12T03:58:30.464655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = range(1, NUM_EPOCHS + 1)\n\nplt.figure()\nplt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\nplt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\")\nplt.xlabel(\"Época\")\nplt.ylabel(\"Pérdida\")\nplt.title(\"Evolución de la pérdida\")\nplt.grid(True)\nplt.legend()\nplt.show()\n\nplt.figure()\nplt.plot(epochs, history[\"train_acc\"], label=\"Train Acc\")\nplt.plot(epochs, history[\"val_acc\"], label=\"Val Acc\")\nplt.xlabel(\"Época\")\nplt.ylabel(\"Exactitud\")\nplt.title(\"Evolución de la exactitud\")\nplt.grid(True)\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T03:58:30.468099Z","iopub.execute_input":"2025-12-12T03:58:30.468419Z","iopub.status.idle":"2025-12-12T03:58:30.861581Z","shell.execute_reply.started":"2025-12-12T03:58:30.468391Z","shell.execute_reply":"2025-12-12T03:58:30.860835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cargar los mejores pesos\nmodel.load_state_dict(torch.load(\"weights/model_best.pth\", map_location=device))\nmodel.eval()\n\nall_labels = []\nall_preds = []\n\nwith torch.no_grad():\n    for inputs, labels in val_loader:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n\n        all_labels.extend(labels.cpu().numpy())\n        all_preds.extend(preds.cpu().numpy())\n\ncm = confusion_matrix(all_labels, all_preds)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\ndisp.plot(cmap=\"Blues\", values_format=\"d\")\nplt.title(\"Matriz de confusión (validación)\")\nplt.show()\n\nprint(\"Reporte de clasificación:\\n\")\nprint(classification_report(all_labels, all_preds, target_names=class_names))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T03:58:30.862488Z","iopub.execute_input":"2025-12-12T03:58:30.862806Z","iopub.status.idle":"2025-12-12T03:58:32.047876Z","shell.execute_reply.started":"2025-12-12T03:58:30.862753Z","shell.execute_reply":"2025-12-12T03:58:32.047198Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchvision.utils as vutils\n\n# Tomar un batch de validación\ninputs, labels = next(iter(val_loader))\ninputs = inputs.to(device)\nlabels = labels.to(device)\n\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(inputs)\n    _, preds = torch.max(outputs, 1)\n\n# Pasar algunas imágenes a CPU para mostrarlas\ninputs_cpu = inputs.cpu()\nlabels_cpu = labels.cpu()\npreds_cpu = preds.cpu()\n\n# Des-normalizar para ver bien\nmean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\nstd = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n\ndef denorm(img):\n    return img * std + mean\n\nfig = plt.figure(figsize=(12, 6))\nfor i in range(8):  # mostrar 8 ejemplos\n    ax = fig.add_subplot(2, 4, i+1)\n    img = denorm(inputs_cpu[i]).clamp(0, 1)\n    npimg = img.permute(1, 2, 0).numpy()\n    ax.imshow(npimg)\n    ax.axis(\"off\")\n    ax.set_title(f\"GT: {class_names[labels_cpu[i]]}\\nPred: {class_names[preds_cpu[i]]}\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T03:58:32.048922Z","iopub.execute_input":"2025-12-12T03:58:32.049536Z","iopub.status.idle":"2025-12-12T03:58:33.613488Z","shell.execute_reply.started":"2025-12-12T03:58:32.049511Z","shell.execute_reply":"2025-12-12T03:58:33.612429Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"-****EXPORTACION A TORCHSCRIPT","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models\n\n# Usaremos CPU para exportación/comparación\ndevice_export = torch.device(\"cpu\")\nprint(\"Device para exportación/comparación:\", device_export)\n\nprint(\"Clases:\", class_names, \"  -> num_classes =\", num_classes)\n\n# Crear la misma arquitectura que entrenaste\nmodel_orig = models.resnet18(weights=None)\nnum_ftrs = model_orig.fc.in_features\nmodel_orig.fc = nn.Linear(num_ftrs, num_classes)\n\n# Cargar los pesos entrenados y dejar el modelo en CPU\nstate_dict = torch.load(\"weights/model_best.pth\", map_location=device_export)\nmodel_orig.load_state_dict(state_dict)\nmodel_orig = model_orig.to(device_export)\nmodel_orig.eval()\n\nprint(\"Modelo original cargado en CPU y listo.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T03:58:33.614595Z","iopub.execute_input":"2025-12-12T03:58:33.614853Z","iopub.status.idle":"2025-12-12T03:58:33.839014Z","shell.execute_reply.started":"2025-12-12T03:58:33.614833Z","shell.execute_reply":"2025-12-12T03:58:33.838250Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nexample_input = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(device_export)\n\n# Exportamos con trace\nts_model = torch.jit.trace(model_orig, example_input)\nts_model = ts_model.to(device_export)\nts_model.eval()\n\nos.makedirs(\"weights\", exist_ok=True)\nts_path = \"weights/model_best_torchscript.pt\"\nts_model.save(ts_path)\n\nprint(f\"Modelo exportado a TorchScript en: {ts_path}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T03:58:33.839851Z","iopub.execute_input":"2025-12-12T03:58:33.840112Z","iopub.status.idle":"2025-12-12T03:58:34.605626Z","shell.execute_reply.started":"2025-12-12T03:58:33.840083Z","shell.execute_reply":"2025-12-12T03:58:34.604840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\n# Tomamos hasta 256 imágenes de validación para la comparación\ninputs_list = []\nlabels_list = []\n\nmax_images = 256\n\nfor batch_inputs, batch_labels in val_loader:\n    inputs_list.append(batch_inputs)\n    labels_list.append(batch_labels)\n    if sum(len(b) for b in labels_list) >= max_images:\n        break\n\ninputs_all = torch.cat(inputs_list, dim=0)[:max_images]\nlabels_all = torch.cat(labels_list, dim=0)[:max_images]\n\nprint(\"Número de imágenes usadas para la comparación:\", len(labels_all))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T03:58:34.606443Z","iopub.execute_input":"2025-12-12T03:58:34.606674Z","iopub.status.idle":"2025-12-12T03:58:35.592035Z","shell.execute_reply.started":"2025-12-12T03:58:34.606649Z","shell.execute_reply":"2025-12-12T03:58:35.591042Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inputs_cpu = inputs_all.to(device_export)\nlabels_cpu = labels_all.to(device_export)\n\nstart = time.time()\nwith torch.no_grad():\n    outputs_orig = model_orig(inputs_cpu)\n    _, preds_orig = torch.max(outputs_orig, 1)\nend = time.time()\n\ntime_orig = end - start\nacc_orig = (preds_orig == labels_cpu).float().mean().item()\n\nprint(f\"Modelo original (PyTorch) -> Accuracy: {acc_orig:.4f}, \"\n      f\"tiempo total: {time_orig:.4f} s, \"\n      f\"tiempo por imagen: {time_orig/len(labels_cpu):.6f} s\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T03:58:35.593312Z","iopub.execute_input":"2025-12-12T03:58:35.593673Z","iopub.status.idle":"2025-12-12T03:58:41.314720Z","shell.execute_reply.started":"2025-12-12T03:58:35.593632Z","shell.execute_reply":"2025-12-12T03:58:41.313671Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cargar modelo TorchScript (por si lo usas en otro entorno después)\nts_model = torch.jit.load(\"weights/model_best_torchscript.pt\", map_location=device_export)\nts_model.eval()\n\nstart = time.time()\nwith torch.no_grad():\n    outputs_ts = ts_model(inputs_cpu)\n    _, preds_ts = torch.max(outputs_ts, 1)\nend = time.time()\n\ntime_ts = end - start\nacc_ts = (preds_ts == labels_cpu).float().mean().item()\n\nprint(f\"Modelo TorchScript -> Accuracy: {acc_ts:.4f}, \"\n      f\"tiempo total: {time_ts:.4f} s, \"\n      f\"tiempo por imagen: {time_ts/len(labels_cpu):.6f} s\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T03:58:41.315684Z","iopub.execute_input":"2025-12-12T03:58:41.316008Z","iopub.status.idle":"2025-12-12T03:58:47.140312Z","shell.execute_reply.started":"2025-12-12T03:58:41.315989Z","shell.execute_reply":"2025-12-12T03:58:47.139572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"coinciden = (preds_orig == preds_ts).float().mean().item()\nprint(f\"Porcentaje de imágenes donde ambos modelos predicen lo mismo: {coinciden*100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T03:58:47.141234Z","iopub.execute_input":"2025-12-12T03:58:47.141494Z","iopub.status.idle":"2025-12-12T03:58:47.145849Z","shell.execute_reply.started":"2025-12-12T03:58:47.141474Z","shell.execute_reply":"2025-12-12T03:58:47.144948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models\n\ndevice_export = torch.device(\"cpu\")\nprint(\"Device export:\", device_export)\n\nprint(\"Clases:\", class_names, \"  -> num_classes =\", num_classes)\n\n# 1. Recrear ResNet18 y cargar pesos\nmodel_orig = models.resnet18(weights=None)\nnum_ftrs = model_orig.fc.in_features\nmodel_orig.fc = nn.Linear(num_ftrs, num_classes)\n\nstate_dict = torch.load(\"weights/model_best.pth\", map_location=device_export)\nmodel_orig.load_state_dict(state_dict)\nmodel_orig = model_orig.to(device_export)\nmodel_orig.eval()\n\n# 2. Exportar a TorchScript con un NOMBRE NUEVO\nexample_input = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(device_export)\nts_model = torch.jit.trace(model_orig, example_input)\n\nts_path = \"weights/model_futbol_ts.pt\"   # <<< nombre nuevo\nts_model.save(ts_path)\n\nprint(\"✅ TorchScript guardado en:\", ts_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T03:59:32.913464Z","iopub.execute_input":"2025-12-12T03:59:32.913756Z","iopub.status.idle":"2025-12-12T03:59:33.699533Z","shell.execute_reply.started":"2025-12-12T03:59:32.913733Z","shell.execute_reply":"2025-12-12T03:59:33.698564Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_ts = torch.jit.load(\"weights/model_futbol_ts.pt\", map_location=\"cpu\")\nprint(\"✅ TorchScript se carga bien en Kaggle:\", type(test_ts))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T03:59:56.850692Z","iopub.execute_input":"2025-12-12T03:59:56.851230Z","iopub.status.idle":"2025-12-12T03:59:56.915923Z","shell.execute_reply.started":"2025-12-12T03:59:56.851204Z","shell.execute_reply":"2025-12-12T03:59:56.915195Z"}},"outputs":[],"execution_count":null}]}